---
title: "Weighted averaged ensembles with genetic algorithms"
author: "Miguel Conde"
date: "21 de noviembre de 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Let's suppose we have a training set with $N$ observations and a target real variable, $y_i$, $i=1...N$.
Upon the $N$ observations we can build $k$ regression models ($M_m$, $m=1,...k$) to estimate each $y_i$ as $\hat{y}_i^m$.

We are interested in building a weighted averaged ensemble this way:

$$
\hat{y}^{ens}_i = \sum_{m=1}^{k}{w_m\hat{y}_i^m}
$$
where:

*  $\hat{y}^{ens}_i$ is the ensemble estimate of $y_i$
*  $\hat{y}_i^m$ is the model $m$ estimate of $y_i$
*  $w_m$ is the weight assigned in the ensemble to the model $m_i$

Let $y_i$ be:
```{r}
N = 100

y <- sin(seq(0, 2*pi, length.out = 100))
```

Let's simulate now the estimates of $k=3$ models.
```{r}
mn1 <- 0
mn2 <- 0.5
mn3 <- -0.5
sd1 <- 0.1
sd2 <- 0.1
sd3 <- 0.1

set.seed(6789)
y_hat <- data.frame(y_hat_m1 = y + rnorm(N, mn1, sd1),
                    y_hat_m2 = y + rnorm(N, mn2, sd2),
                    y_hat_m3 = y + rnorm(N, mn3, sd3))
```

We'll use this code:
```{r, message=FALSE, warning=FALSE}
source("gaWARE.R")
```

Let's check the fitness function:

```{r}
# Check
fitness_gaWARE(w = c(1, 0, -1), y = y, y_hat = y_hat,
               maximize = TRUE, perfFunc = rmse)

fitness_gaWARE(w = c(1, 0, -1), y = y, y_hat = y_hat,
               maximize = FALSE, perfFunc = myMAE)
```

Okay, let's find our ensemble parameters:

```{r}
# Ranges of parameters to estimate
minVector <- c(-10, -10, -10) # w1, w2, w3
maxVector <- c(10, 10, 10)

# Optimization (In this example: COST FUNCTION = myMAE())
GA <- ga(type = "real-valued",
         fitness = function(x, ...) fitness_gaWARE(x, ...),
         ## '...' for fitness_gaWARE()
         y = y, y_hat = y_hat,
         maximize = FALSE, perfFunc = myMAE,
         ##
         min = minVector, max = maxVector, popSize = 50,
         maxiter = 100, names = c("w1", "w2", "w3"),
         seed = 789,
         monitor = NULL)

summary(GA)
GA@solution
```

```{r include=FALSE}
w <- drop(GA@solution)
```

And our ensemble is:

$$
\hat{y}^{ens}_i = \sum_{m=1}^{k}{w_m\hat{y}_i^m} = `r w[1]`\times\hat{y}_1 + `r w[2]`\times\hat{y}_2 + `r w[2]`\times\hat{y}_3 
$$

```{r}
y_hat_ens <- as.matrix(y_hat) %*% t(GA@solution)
```


## Using cross validation

```{r}
cvGA <- cvOpt_gaWARE(type = "cv", numFolds = 10, seed = 345,
                     y = y, y_hat = y_hat, maximize = FALSE, 
                     perfFunc = myMAE,
                     min = minVector, max = maxVector,
                     popsize = 50, maxiter = 100, 
                     names = c("w1", "w2", "w3"),
                     monitor = NULL)
```


And now:
$$
\hat{y}^{ens}_i = \sum_{m=1}^{k}{w_m\hat{y}_i^m} = `r cvGA$mn[1]`\times\hat{y}_1 + `r cvGA$mn[2]`\times\hat{y}_2 + `r cvGA$mn[2]`\times\hat{y}_3 
$$
```{r}
y_hat_ens <- as.matrix(y_hat) %*% as.matrix(cvGA$mn)
head(y_hat_ens)
```

It's better to use the appropiated function:

A) With CV average weights:
```{r}
y_hat_ens <- predict.gaWARE(optGA = cvGA, x = y_hat, return_avg_wts = TRUE)
head(y_hat_ens)
```

B) Or with the best CV set of weights:
```{r}
y_hat_ens <- predict.gaWARE(optGA = cvGA, x = y_hat, return_avg_wts = FALSE)
head(y_hat_ens)
```


As a curiosity:
```{r}
cvGA[setdiff(names(cvGA), c("GA"))]
```


# Session Info
```{r}
sessionInfo()
```




